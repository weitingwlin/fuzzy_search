{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = [16,12]\n",
    "\n",
    "# from sklearn import manifold\n",
    "from itertools import product, combinations\n",
    "from nltk.corpus import stopwords\n",
    "# from scipy.stats import rankdata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read\n",
    "books = pd.read_csv('fiction.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors, Phrases\n",
    "filename = '/Users/weitinglin/Documents/GoogleNews-vectors-negative300.bin'\n",
    "model = KeyedVectors.load_word2vec_format(filename, binary=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# partial model\n",
    "# preload\n",
    "# data base\n",
    "# machine\n",
    "# vec = model.vectors\n",
    "\n",
    "\n",
    "# len(model.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trim_string(S):\n",
    "    '''\n",
    "    trim useless words if string is too long\n",
    "    '''\n",
    "    mystr1 = re.split('[\\W\\s]+', S)\n",
    "    # split at punctuation or space\n",
    "    \n",
    "    mystr =[s.lower() for s in mystr1] \n",
    "    # Remove \"the\", \"a\", \"an\"\n",
    "    nonsense = [\"the\", \"a\", \"an\", \"and\", \"to\",\"on\", \"from\", \"in\", \"by\"]\n",
    "    mystr = [word for word in mystr if word.lower() not in nonsense]\n",
    "        \n",
    "    # remove more\n",
    "    mystr_less = [word for word in mystr if word.lower() not in stopwords.words('english')]\n",
    "    \n",
    "    if len(mystr_less) > 0 :\n",
    "        mystr = mystr_less\n",
    "    \n",
    "    # remove placeholder \n",
    "    mystr_less = [s for s in mystr if s in model.vocab]\n",
    "    if len(mystr_less) > 0 :\n",
    "        mystr = mystr_less\n",
    "    \n",
    "    return mystr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['slaughterhouse', 'five']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = trim_string('Slaughterhouse-Five')\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def str2mat(instr, limit = 5, placeholder = None):\n",
    "    '''\n",
    "    Convert string to a vector base on average vector of the composing words.\n",
    "    instr: the inpput string\n",
    "    placeholder: for the non-vocabularies\n",
    "    '''\n",
    "    # make a place-holder: mean of three strange words\n",
    "    if placeholder is None:\n",
    "        ph = (model.get_vector(\"Ka_wai\") + \\\n",
    "              model.get_vector(\"Chief_Carla_Piluso\") + model.get_vector(\"Marc_Andre_Bergeron\"))/1\n",
    "    \n",
    "    mystr = trim_string(instr)\n",
    "    \n",
    "    # number of words\n",
    "    L = min(len(mystr), limit) \n",
    "    \n",
    "    ## padding up\n",
    "    sheet = np.ones((300, limit))* 2 \n",
    "    for l in range(L):\n",
    "        if (mystr[l] in model.vocab):\n",
    "            sheet[:,l] = model.get_vector(mystr[l])\n",
    "        else:\n",
    "            sheet[:,l] = ph\n",
    "  \n",
    "    return L, sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compare_mats(M1, M2 , ph = 2, stress = 0.2, penalty = 0.5):\n",
    "    n, limit = M1.shape\n",
    "    L1 = sum(M1[0,:] != 2 ) # lenth of \n",
    "    L2 = sum(M2[0,:] != 2 ) \n",
    "    # trim \n",
    "    M1_trim = M1[:, 0:L1]\n",
    "    M2_trim = M2[:, 0:L2]\n",
    "    \n",
    "    if L1 == 1:\n",
    "        lin_dist = M2_trim - M1_trim\n",
    "        euc_dist = [np.sqrt(sum(lin_dist[:,i]  ** 2)) for i in range(L2)]\n",
    "        dist = min(euc_dist)\n",
    "    elif L2 == 1:\n",
    "        lin_dist = np.tile(M2_trim, L1) - M1_trim\n",
    "        euc_dist = [np.sqrt(sum(lin_dist[:,i]  ** 2)) for i in range(L1)]\n",
    "        # use mean if target is more than 1 words\n",
    "        dist = np.mean(euc_dist) \n",
    "    else:\n",
    "        # \n",
    "        ind_product = list(product(np.arange(L2), repeat=L1)) # select from M2 to match the size of M1\n",
    "        ind_combination = list(combinations(np.arange(L2), L1)) \n",
    "        eucs = []\n",
    "        for p, ind in enumerate(ind_product): # 2, (0,1):\n",
    "            M2_p = M2_trim[:,list(ind)] # permuted M2'\n",
    "            lin_dist = M2_p - M1_trim\n",
    "            euc = [np.sqrt(sum(lin_dist[:,i]  ** 2)) for i in range(L1)]\n",
    "            mean_euc = np.mean(euc)\n",
    "            if ind not in ind_combination:\n",
    "                mean_euc = mean_euc * (stress + 1)\n",
    "            eucs.append(mean_euc)\n",
    "        dist = min(eucs)\n",
    "    # penalty for unequal list\n",
    "    if L2 > L1:\n",
    "        dist = dist +  ((L2 - L1)/(L1 + L2) * penalty)\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compare_strs(S1, S2, limit = 5, placeholder = None):\n",
    "    _, M1 = str2mat(S1, limit = limit)\n",
    "    _, M2 = str2mat(S2, limit = limit)\n",
    "    return compare_mats(M1, M2 , ph = placeholder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16666666666666666"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_strs('dark', 'Dark tower')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.7817500703577505"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_strs('dark tower', 'ddddddd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.131742691043174"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_strs('dark tower', 'dragonfly in amber')# 3.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.32972583831502"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_strs('the dark tower', 'black castle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fuzzy_find2(mytitle, shelf, maxshow = 10, threshhold = 5):\n",
    "    '''\n",
    "    mytitle: the user input keyword for fuzzy search\n",
    "    shelf: df with column named 'title', find book from\n",
    "    maxshow: the max. number of result return.\n",
    "    threshhold: threshhold of similarity for the \"match\"\n",
    "    '''\n",
    "    dist = []\n",
    "    for s in shelf[\"title\"]:\n",
    "        dist.append(compare_strs(mytitle, s))\n",
    "    dist = np.array(dist)\n",
    "    \n",
    "    fuzzy = np.where(dist < threshhold)[0]\n",
    "    L = len(fuzzy)\n",
    "    if L > maxshow:\n",
    "        rankF = rankdata(dist, method='min') \n",
    "        fuzzy = np.where(rankF <= maxshow)[0]\n",
    "\n",
    "    return shelf[\"title\"][fuzzy], dist[fuzzy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10                 See Me\n",
       " 14                     It\n",
       " 54               Still Me\n",
       " 64    Dragonfly in Amber \n",
       " 93           We Are Water\n",
       " Name: title, dtype: object,\n",
       " array([3.78670202, 3.61952804, 3.76593499, 3.57965192, 3.8536134 ]))"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzzy_find2('butterfly in resin', books, maxshow=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14                             It\n",
       " 48               The Dark Tower I\n",
       " 54                       Still Me\n",
       " 57    All the Light We Cannot See\n",
       " 59        The Women in the Castle\n",
       " 73             At Home in Mitford\n",
       " Name: title, dtype: object,\n",
       " array([3.08585741, 1.96243112, 3.17527289, 3.19249795, 1.70018074,\n",
       "        3.21407874]))"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzzy_find2('Dark castle', books, maxshow=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## to do\n",
    "bring me to the link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## notes:\n",
    "update str2mat so the order matters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a smaller model\n",
    "#### 1. get a list of common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "file = \"/Users/weitinglin/Documents/google-20000-english-usa.txt\"\n",
    "with open(file, 'r') as f:\n",
    "    x = f.readlines()\n",
    "\n",
    "common = [w.strip() for w in x if w.strip() != \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# file = \"/Users/weitinglin/Documents/wiki-100k.txt\"\n",
    "# with open(file, 'r') as f:\n",
    "#     x = f.readlines()\n",
    "\n",
    "# morecommon = [w.strip() for w in x if w.strip() != \"\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make my own dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fiction = pd.read_csv('fiction_500.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_dictionary(books):\n",
    "    temp = []\n",
    "    for b in books:\n",
    "        temp = temp + re.split('[\\W\\s]+', b.lower()) \n",
    "    return set(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict1 = list(make_dictionary(fiction['title']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title'}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "more_common = set(common + dict1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "### make a dictionary\n",
    "common_dict = {}\n",
    "for w in more_common:\n",
    "    if w in model.vocab:\n",
    "        common_dict[w] = model.get_vector(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "dill.dump(common_dict, open(\"morecommon_dict.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# all the above only have to do once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# c_dict = dill.load( open(\"common_dict.pkl\",\"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mock up the web app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import dill\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "# plt.rcParams[\"figure.figsize\"] = [16,12]\n",
    "\n",
    "# from sklearn import manifold\n",
    "from itertools import product, combinations\n",
    "from nltk.corpus import stopwords\n",
    "from scipy.stats import rankdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c_dict = dill.load( open(\"morecommon_dict.pkl\",\"rb\"))# 20k + my_dict\n",
    "# c_dict = dill.load( open(\"common_dict.pkl\",\"rb\"))\n",
    "vocab = c_dict.keys()\n",
    "books = pd.read_csv('fiction.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18883"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the string trimming function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trim_string(S):\n",
    "    '''\n",
    "    trim useless words if string is too long\n",
    "    '''\n",
    "    mystr1 = re.split('[\\W\\s]+', S)\n",
    "    # split at punctuation or space\n",
    "    \n",
    "    mystr =[s.lower() for s in mystr1] \n",
    "    # Remove \"the\", \"a\", \"an\"\n",
    "    nonsense = [\"the\", \"a\", \"an\", \"and\", \"to\",\"on\", \"from\", \"in\", \"by\"]\n",
    "    mystr = [word for word in mystr if word.lower() not in nonsense]\n",
    "        \n",
    "    # remove more\n",
    "    mystr_less = [word for word in mystr if word.lower() not in stopwords.words('english')]\n",
    "    \n",
    "    if len(mystr_less) > 0 :\n",
    "        mystr = mystr_less\n",
    "    \n",
    "    # remove placeholder \n",
    "    mystr_less = [s for s in mystr if s in vocab]\n",
    "    if len(mystr_less) > 0 :\n",
    "        mystr = mystr_less\n",
    "        \n",
    "    return mystr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['slaughterhouse', 'five']"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = trim_string('Slaughterhouse-Five')\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dragonfly', 'amber']"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trim_string('dragonfly in amber')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18883"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_dict['banana'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def str2mat(instr, limit = 5, placeholder = None):\n",
    "    '''\n",
    "    Convert string to a vector base on average vector of the composing words.\n",
    "    instr: the inpput string\n",
    "    placeholder: for the non-vocabularies\n",
    "    '''\n",
    "    # make a place-holder: mean of three strange words\n",
    "    if placeholder is None:\n",
    "        ph = np.ones(300)* 3\n",
    "    \n",
    "    mystr = trim_string(instr)\n",
    "    \n",
    "    # number of words\n",
    "    L = min(len(mystr), limit) \n",
    "    \n",
    "    ## padding up\n",
    "    sheet = np.ones((300, limit))* 2 \n",
    "    for l in range(L):\n",
    "        if (mystr[l] in vocab):\n",
    "            sheet[:,l] = c_dict[mystr[l]]\n",
    "        else:\n",
    "            sheet[:,l] = ph\n",
    "  \n",
    "    return L, sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, array([[ 3.44238281e-02,  1.77764893e-03,  2.00000000e+00,\n",
       "          2.00000000e+00,  2.00000000e+00],\n",
       "        [ 8.98437500e-02, -3.51562500e-02,  2.00000000e+00,\n",
       "          2.00000000e+00,  2.00000000e+00],\n",
       "        [-2.83203125e-01,  1.12304688e-01,  2.00000000e+00,\n",
       "          2.00000000e+00,  2.00000000e+00],\n",
       "        ...,\n",
       "        [-5.51757812e-02,  1.16699219e-01,  2.00000000e+00,\n",
       "          2.00000000e+00,  2.00000000e+00],\n",
       "        [-4.98046875e-02,  2.08007812e-01,  2.00000000e+00,\n",
       "          2.00000000e+00,  2.00000000e+00],\n",
       "        [-2.45666504e-03, -1.25000000e-01,  2.00000000e+00,\n",
       "          2.00000000e+00,  2.00000000e+00]]))"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str2mat('dragonfly in amber')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compare_mats(M1, M2 , ph = 2, stress = 0.2, penalty = 0.5):\n",
    "    n, limit = M1.shape\n",
    "    L1 = sum(M1[0,:] != 2 ) # lenth of \n",
    "    L2 = sum(M2[0,:] != 2 ) \n",
    "    # trim \n",
    "    M1_trim = M1[:, 0:L1]\n",
    "    M2_trim = M2[:, 0:L2]\n",
    "    \n",
    "    if L1 == 1:\n",
    "        lin_dist = M2_trim - M1_trim\n",
    "        euc_dist = [np.sqrt(sum(lin_dist[:,i]  ** 2)) for i in range(L2)]\n",
    "        dist = min(euc_dist)\n",
    "    elif L2 == 1:\n",
    "        lin_dist = np.tile(M2_trim, L1) - M1_trim\n",
    "        euc_dist = [np.sqrt(sum(lin_dist[:,i]  ** 2)) for i in range(L1)]\n",
    "        # use mean if target is more than 1 words\n",
    "        dist = np.mean(euc_dist) \n",
    "    else:\n",
    "        # \n",
    "        ind_product = list(product(np.arange(L2), repeat=L1)) # select from M2 to match the size of M1\n",
    "        ind_combination = list(combinations(np.arange(L2), L1)) \n",
    "        eucs = []\n",
    "        for p, ind in enumerate(ind_product): # 2, (0,1):\n",
    "            M2_p = M2_trim[:,list(ind)] # permuted M2'\n",
    "            lin_dist = M2_p - M1_trim\n",
    "            euc = [np.sqrt(sum(lin_dist[:,i]  ** 2)) for i in range(L1)]\n",
    "            mean_euc = np.mean(euc)\n",
    "            if ind not in ind_combination:\n",
    "                mean_euc = mean_euc * (stress + 1)\n",
    "            eucs.append(mean_euc)\n",
    "        dist = min(eucs)\n",
    "    # penalty for unequal list\n",
    "    if L2 > L1:\n",
    "        dist = dist +  ((L2 - L1)/(L1 + L2) * penalty)\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compare_strs(S1, S2, limit = 5, placeholder = None):\n",
    "    _, M1 = str2mat(S1, limit = limit)\n",
    "    _, M2 = str2mat(S2, limit = limit)\n",
    "    return compare_mats(M1, M2 , ph = placeholder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fuzzy_find2(mytitle, shelf, maxshow = 10, threshhold = 5):\n",
    "    '''\n",
    "    mytitle: the user input keyword for fuzzy search\n",
    "    shelf: df with column named 'title', find book from\n",
    "    maxshow: the max. number of result return.\n",
    "    threshhold: threshhold of similarity for the \"match\"\n",
    "    '''\n",
    "    dist = []\n",
    "    for s in shelf[\"title\"]:\n",
    "        dist.append(compare_strs(mytitle, s))\n",
    "    dist = np.array(dist)\n",
    "    \n",
    "    fuzzy = np.where(dist < threshhold)[0]\n",
    "    L = len(fuzzy)\n",
    "    if L > maxshow:\n",
    "        rankF = rankdata(dist, method='min') \n",
    "        fuzzy = np.where(rankF <= maxshow)[0]\n",
    "\n",
    "#     return shelf[\"title\"][fuzzy], dist[fuzzy]\n",
    "    return list(shelf[\"title\"][fuzzy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = fuzzy_find2('Butterfly in resin', books, maxshow=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['See Me', 'It', 'Still Me', 'Dragonfly in Amber ', 'We Are Water']"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## todo\n",
    "* data-specific dictionary\n",
    "* trim ()/audio colection\n",
    "* penalty to stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fiction = pd.read_csv('fiction_500.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_dictionary(books):\n",
    "    temp = []\n",
    "    for b in books:\n",
    "        temp = temp + re.split('[\\W\\s]+', b.lower()) \n",
    "    return set(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_dict = list(make_dictionary(fiction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
